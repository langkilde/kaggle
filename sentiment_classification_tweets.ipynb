{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Sentiment classification on tweets about airlines\n\nThis notebook describes an attempt to classify tweets by sentiment. It describes the initial data exploration, as well as implementation of a classifier.\n\nFirst we start by importing some necessary tools.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## What is in the dataset?\n\nIt's always good to start by exploring the data that we have available. To do this we load the raw csv file using [Pandas][1] and check what the columns are.\n\n  [1]: http://pandas.pydata.org/",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nrawData = pd.read_csv(\"../input/Tweets.csv\")\nlist(rawData.columns.values)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We want to be able to determine the sentiment of a tweet without any other information but the tweet text itself, hence the 'text' column is our focus. Using the text we are going to try and predict 'airline_sentiment'. We also need to take into account 'airline_sentiment_confidence', but we will come back to that.\n\nLets take a look at what a typical record looks like.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rawData.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets take a look at what sentiments have been found.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sentiment_counts = rawData.airline_sentiment.value_counts()\nnumber_of_tweets = rawData.tweet_id.count()\nprint(sentiment_counts)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It turns out that our dataset is skewed with significantly more negative than positive tweets. We will focus on the issue of separating positive and negative tweets. It's good to keep in mind that, while a terrible classifier, if we always guessed a tweet was negative we'd be right 79% of the time (9178 of 11541). That clearly wouldn't be a very useful classifier, but worth to remember.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Let's explore the text\n\nWe begin by checking what common words we can find in each of the different classes. To investigate this we want to preprocess our data a little. Let's get rid of the 100 most common words, and some punctuation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We need some ugly code to supress deprecation warnings resulting from nltk on Kaggle\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# What characterizes text of different sentiments?\n\nWhile we still haven't decided what classification method to use, it's useful to get an idea of how the different texts look. This might be an \"old school\" approach in the age of deep learning, but lets indulge ourselves nevertheless. \n\nTo explore the data we apply some crude preprocessing. We will tokenize and lemmatize using [Python NLTK][1], and transform to lower case. As words mostly matter in context we'll look at bi-grams instead of just individual tokens.\n\n### Preprocessing\nNote that we remove the first two tokens as they always contain \"@ airline_name\".\n\n\n  [1]: http://www.nltk.org/",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re, nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\nnegative_tweets = rawData.loc[rawData['airline_sentiment'] == 'negative'].text\ndef normalize_tweet(tweet):\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    tokens = nltk.word_tokenize(only_letters)\n    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in tokens[2:]]\n    lower_case = [l.lower() for l in lemmas]\n    filtered_result = list(filter(lambda l: l not in stop_words,lower_case))\n    return filtered_result\n\npreprocessed_negative_tweets = negative_tweets.apply(normalize_tweet)\nprint('Example preprocessed tweet:\\n', preprocessed_negative_tweets.iloc[0])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk import ngrams\ndef grams(tokens):\n    return list(ngrams(tokens, 3))\nnegative_grams = preprocessed_negative_tweets.apply(grams)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And now some counting.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import collections\ndef count_words(input):\n    cnt = collections.Counter()\n    for row in input:\n        for word in row:\n            cnt[word] += 1\n    return cnt\n\ncount_words(negative_grams).most_common(20)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can already tell there's a pattern here. Sentences like \"cancelled flight\", \"late flight\", \"booking problems\",  \"delayed flight\" stand out clearly. Lets check the positive tweets.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "positive_tweets = rawData.loc[rawData['airline_sentiment'] == 'positive'].text\npreprocessed_positive_tweets = positive_tweets.apply(normalize_tweet)\npositive_grams = preprocessed_positive_tweets.apply(grams)\ncount_words(positive_grams).most_common(20)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Some more good looking patterns here. We can however see that with 3-grams clear patterns are rare. \"great customer service\" occurs 12 times in 2362 positive responses, which really doesn't say much in general. \n\nSatisfied that our data looks possible to work with begin to construct our first classifier.\n\n# First Classifier\nLets start simple with a bag-of-words Support-Vector-Machine (SVM) classifier. Bag-of-words means that we represent each sentence by the unique words in it. To make this representation useful for our SVM classifier we transform each sentence into a vector. The vector is of the same length as our vocabulary, i.e. the list of all words observed in our training data, with each word representing an entry in the vector. If a particular word is present, that entry in the vector is 1, otherwise 0.\n\nTo create these vectors we use the CountVectorizer from [sklearn][1]. Note that we make sure to have an index that mapps vectorized data back to the original sentence. This will come in handy when inspecting output from the classifier later.\n\n\n  [1]: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Preparing the data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\nnegative_data = preprocessed_negative_tweets.apply(' '.join).as_matrix().tolist()\npositive_data = preprocessed_positive_tweets.apply(' '.join).as_matrix().tolist()\nnegative_targets = np.zeros((len(negative_data),1))\npositive_targets = np.ones((len(positive_data),1))\nraw_data = negative_data+positive_data\nvectorized_data = count_vectorizer.fit_transform(raw_data)\ntargets = np.concatenate((negative_targets,positive_targets), axis=0).ravel()\nindexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To check performance of our classifier we want to split our data in to train and test.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ndata_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\ndata_train_index = data_train[:,0]\ndata_train = data_train[:,1:]\ndata_test_index = data_test[:,0]\ndata_test = data_test[:,1:]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Fitting a classifier\n\nWe're now ready to fit a classifier to our data. We'll spend more time on hyper parameter tuning later, so for now we just pick some reasonable guesses.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn import svm\nclf = svm.SVC(gamma=0.01, C=100., probability=True)\nclf_settings = clf.fit(data_train, targets_train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Evaluation of results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "clf.score(data_test, targets_test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "87% likely isn't great, but it's not nothing. It's most likely possible to achieve a higher score with more tuning, or a more advanced approach. Lets check on how it does on a couple of sentences.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sentences = count_vectorizer.transform([\n    \"What a great airline, the trip was a pleasure!\",\n    \"My issue was quickly resolved after calling customer support. Thanks!\",\n    \"What the hell! My flight was cancelled again. This sucks!\",\n    \"Service was awful. I'll never fly with you again.\",\n    \"You fuckers lost my luggage. Never again!\",\n    \"I have mixed feelings about airlines. I don't know what I think.\",\n    \"\"\n])\nclf.predict_proba(sentences)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So while not a huge improvement over the baseline, we can see that it's doing a good job on these obvious sentences. \n\n## What is hard for the classifier?\n\nIt's interesting to know which sentences are hard. To find out, lets apply the classifier to all our test sentences and sort by the marginal probability.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predictions_on_test_data = np.array(clf.predict_proba(data_test))\nindex = np.transpose(np.array([range(0,len(predictions_on_test_data))]))\nindexed_predictions = np.concatenate((predictions_on_test_data, index), axis=1).tolist()\nhardest_test_sentences = sorted(list(map(lambda p : [abs(p[0]-p[1]), p[2]], indexed_predictions)), key=lambda p : p[0])\nlist(map(lambda p : raw_data[data_test_index[p[1]].toarray()[0][0]], hardest_test_sentences[0:20]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "How about the easiest test sentences?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list(map(lambda p : raw_data[data_test_index[p[1]].toarray()[0][0]], hardest_test_sentences[-20:]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list(map(lambda p : clf.predict_proba(data_test[p[1]]), hardest_test_sentences[-20:]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}